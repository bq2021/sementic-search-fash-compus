{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff02fe2a-a5f3-4a26-938f-3408a1011a78",
   "metadata": {},
   "source": [
    "### 1주차 과제 진행\n",
    "* client.ipynb는 카프카에 요청 위한 코드\n",
    "* 테스트 시 server.ipynb 실행, client.ipynb 실행 후 색인 진행\n",
    "\n",
    "1. 제품 listing 전체를 Elasticsearch에 인덱스\n",
    "2. 제품 인덱스의 ”item_keywords” 와 “product_description”를 업데이트 할 수 있는 API\n",
    "3. item_keyword와 product_description 정보로 제품을 검색\n",
    "---\n",
    "세부사항\n",
    "Kafka\n",
    "Elasticsearch\n",
    "- 로컬환경에서 작동되는 인프라를 구축합니다.(텍스트 전처리는 python의 nltk를 씁니다: https://www.nltk.org/)\n",
    "- 다량의 업데이트를 지원할 수 있는 시스템 구축을 가정합니다. (Flink: https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/dev/python/overview/)\n",
    "- language_tag가 en_US 로 된 제품만 인덱스 합니다. (Kafka: https://docs.confluent.io/kafka-clients/python/current/overview.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "968261a1-079d-4946-a277-fdcbed9739b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# ~/nltk_data에 말뭉치 저장.\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt_tab')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "def analyze_document_field(doc_field):\n",
    "    if doc_field is None:\n",
    "        return []  \n",
    "        \n",
    "    # 토크나이징\n",
    "    word_tokens = word_tokenize(doc_field)\n",
    "    \n",
    "    # 불용어 처리\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_words = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "    \n",
    "    # 어간 추출\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_words = [stemmer.stem(word) for word in filtered_words]\n",
    "    \n",
    "    # 기본형 추출\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
    "    return ' '.join(lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cdc528a-f9f7-4968-acd3-56d08db0058a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/11 13:57:49 WARN Utils: Your hostname, HG380-MacBookAir.local resolves to a loopback address: 127.0.0.1; using 192.168.182.66 instead (on interface en0)\n",
      "24/10/11 13:57:49 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "24/10/11 13:57:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col,explode, expr\n",
    "\n",
    "# SparkSession 생성\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Read GZ Compressed JSON Files with PySpark\") \\\n",
    "    .config(\"spark.sql.debug.maxToStringFields\", \"1000\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fbc4c0f-a77c-484f-be13-94f4333ed536",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# gzip으로 압축된 JSON 파일들 읽기 (예: /path/to/json/*.json.gz 경로의 모든 파일)\n",
    "json_path = \"abo-listings/listings/metadata/*.json.gz\"\n",
    "df = spark.read.json(json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f97dc8c-7e47-43ae-9641-3a32e8304ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Write the DataFrame to an Elasticsearch index\n",
    "es_conf = {\n",
    "    \"es.nodes.discovery\": \"false\",\n",
    "    \"es.nodes.data.only\": \"false\",\n",
    "    \"es.net.http.auth.user\": \"elastic\",\n",
    "    \"es.net.http.auth.pass\": \"password\",\n",
    "    \"es.index.auto.create\": \"true\",\n",
    "    \"es.nodes\": \"http://127.0.0.1\",\n",
    "    \"es.port\": \"9200\",\n",
    "    \"es.mapping.id\": \"item_id\",\n",
    "    \"es.nodes.wan.only\": \"true\"\n",
    "}\n",
    "\n",
    "df.write.mode(\"append\") \\\n",
    "        .format('org.elasticsearch.spark.sql') \\\n",
    "        .options(**es_conf) \\\n",
    "        .save(\"amazon-berkeley\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49acb4f0-970e-48b7-9927-5b1b044ef470",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json, col, explode, collect_list\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"item_id\", StringType(), True),\n",
    "    StructField(\"item_keywords\", ArrayType(StructType([\n",
    "        StructField(\"language_tag\", StringType(), True),\n",
    "        StructField(\"value\", StringType(), True)\n",
    "    ])), True),\n",
    "    StructField(\"product_description\", ArrayType(StructType([\n",
    "        StructField(\"language_tag\", StringType(), True),\n",
    "        StructField(\"value\", StringType(), True)\n",
    "    ])), True)\n",
    "])\n",
    "kafka_stream = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"amazon_berkeley_update\") \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .option(\"failOnDataLoss\", \"false\") \\\n",
    "    .load()\n",
    "\n",
    "json_stream = kafka_stream.selectExpr(\"CAST(value AS STRING)\") \\\n",
    "    .select(from_json(\"value\", schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb08c765-9e84-4b18-8a1f-6c3a2ef2b187",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/11 14:02:29 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "24/10/11 14:02:33 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "es_conf = {\n",
    "    \"es.nodes.discovery\": \"false\",\n",
    "    \"es.nodes.data.only\": \"false\",\n",
    "    \"es.net.http.auth.user\": \"elastic\",\n",
    "    \"es.net.http.auth.pass\": \"password\",\n",
    "    \"es.nodes\": \"http://127.0.0.1\",\n",
    "    \"es.port\": \"9200\",\n",
    "    \"es.mapping.id\": \"item_id\",\n",
    "    \"es.mapping.exclude\": \"item_id\",\n",
    "    \"es.write.operation\": \"update\",\n",
    "    \"checkpointLocation\": \"/tmp/\",\n",
    "    \"es.spark.sql.streaming.sink.log.enabled\": \"true\",\n",
    "    # \"failOnDataLoss\": \"false\",\n",
    "    \"es.nodes.wan.only\": \"true\"\n",
    "}\n",
    "\n",
    "# Filter the \"en_US\" language tag for item_keywords and product_description\n",
    "def filter_en_us(df, epoch_id):\n",
    "    # Explode the array columns to filter them\n",
    "    keywords_df = df.withColumn(\"item_keywords\", explode(col(\"item_keywords\"))) \\\n",
    "                    .filter(col(\"item_keywords.language_tag\") == \"en_US\") \\\n",
    "                    .groupBy(\"item_id\") \\\n",
    "                    .agg(collect_list(\"item_keywords\").alias(\"item_keywords\"))\n",
    "\n",
    "    description_df = df.withColumn(\"product_description\", explode(col(\"product_description\"))) \\\n",
    "                       .filter(col(\"product_description.language_tag\") == \"en_US\") \\\n",
    "                       .groupBy(\"item_id\") \\\n",
    "                       .agg(collect_list(\"product_description\").alias(\"product_description\"))\n",
    "\n",
    "    # Join the two DataFrames back on item_id\n",
    "    final_df = keywords_df.join(description_df, \"item_id\", \"inner\")\n",
    "\n",
    "    # Write to Elasticsearch\n",
    "    final_df.write.mode(\"append\") \\\n",
    "        .format('org.elasticsearch.spark.sql') \\\n",
    "        .options(**es_conf) \\\n",
    "        .save('amazon-berkeley')\n",
    "\n",
    "# Start the streaming query to process the incoming JSON stream\n",
    "query = json_stream.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .foreachBatch(filter_en_us) \\\n",
    "    .options(**es_conf) \\\n",
    "    .start(\"amazon-berkeley\")\n",
    "\n",
    "query.awaitTermination()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python-3.9",
   "language": "python",
   "name": "python-3.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
